import os
import io
import uuid
import threading
import queue
import torch
import torchvision.transforms as T
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import requests

# --- C√ÅC H√ÄM TI·ªÄN X·ª¨ L√ù ·∫¢NH ---
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    """X√¢y d·ª±ng pipeline chuy·ªÉn ƒë·ªïi ·∫£nh."""
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    """T√¨m t·ª∑ l·ªá khung h√¨nh g·∫ßn nh·∫•t v·ªõi ·∫£nh g·ªëc."""
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªông, chia ·∫£nh th√†nh c√°c patches."""
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # T√≠nh to√°n c√°c t·ª∑ l·ªá khung h√¨nh m·ª•c ti√™u
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # T√¨m t·ª∑ l·ªá khung h√¨nh g·∫ßn nh·∫•t
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # T√≠nh to√°n k√≠ch th∆∞·ªõc m·ª•c ti√™u
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # Resize ·∫£nh
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # Chia ·∫£nh th√†nh c√°c ph·∫ßn
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image(image_data, input_size=448, max_num=6):
    """T·∫£i v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh t·ª´ bytes data."""
    
    # Chuy·ªÉn ƒë·ªïi bytes th√†nh PIL Image
    if isinstance(image_data, bytes):
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
    else:
        image = Image.open(image_data).convert('RGB')
    
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# --- C·∫§U H√åNH GLOBAL ---
# ƒê∆Ø·ªúNG D·∫™N MODEL - T·ª± ƒë·ªông ph√°t hi·ªán m√¥i tr∆∞·ªùng
MODEL_NAME = "5CD-AI/Vintern-1B-v3_5"

# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n Docker tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ th√¨ d√πng ƒë∆∞·ªùng d·∫´n local
DOCKER_MODEL_PATH = "/app/internvl_local"
LOCAL_MODEL_PATH = "internvl_local"

# Ch·ªçn ƒë∆∞·ªùng d·∫´n ph√π h·ª£p
if os.path.exists(DOCKER_MODEL_PATH):
    LOCAL_MODEL_PATH = DOCKER_MODEL_PATH
elif not os.path.exists(LOCAL_MODEL_PATH):
    # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng t·ªìn t·∫°i, t·∫°o th∆∞ m·ª•c local
    os.makedirs(LOCAL_MODEL_PATH, exist_ok=True)

LOCAL_MODEL_PATH = os.path.abspath(LOCAL_MODEL_PATH) 

# Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng model v√† tokenizer r·ªóng
model = None
tokenizer = None

# T·ª± ƒë·ªông ph√°t hi·ªán device (GPU ho·∫∑c CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîç S·ª≠ d·ª•ng device: {device}")

# Queue system ƒë·ªÉ x·ª≠ l√Ω request tu·∫ßn t·ª± (v√¨ ch·ªâ c√≥ 1 CPU)
# C·∫£i thi·ªán: D√πng Event thay v√¨ polling
request_queue = queue.Queue()
processing_lock = threading.Lock()
result_store = {}  # L∆∞u k·∫øt qu·∫£ theo request_id
result_lock = threading.Lock()
request_events = {}  # Event ƒë·ªÉ signal khi request xong
event_lock = threading.Lock()

# Kh·ªüi t·∫°o Flask app v·ªõi CORS
app = Flask(__name__)
CORS(app)  # Cho ph√©p t·∫•t c·∫£ origins, c√≥ th·ªÉ c·∫•u h√¨nh chi ti·∫øt h∆°n n·∫øu c·∫ßn

def load_model():
    """T·∫£i m√¥ h√¨nh l√™n device (GPU ho·∫∑c CPU) m·ªôt l·∫ßn duy nh·∫•t khi server kh·ªüi ƒë·ªông."""
    global model, tokenizer, device
    
    # ƒê·∫£m b·∫£o device ƒë∆∞·ª£c ph√°t hi·ªán l·∫°i (ph√≤ng tr∆∞·ªùng h·ª£p thay ƒë·ªïi sau khi import)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üîç ƒêang t·∫£i model l√™n device: {device}")
    if device == "cuda":
        print(f"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            LOCAL_MODEL_PATH, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        # Ch·ªçn dtype ph√π h·ª£p v·ªõi device
        # GPU: d√πng bfloat16 (nhanh, ti·∫øt ki·ªám VRAM)
        # CPU: d√πng float32 (t∆∞∆°ng th√≠ch t·ªët)
        dtype = torch.bfloat16 if device == "cuda" else torch.float32
        print(f"   S·ª≠ d·ª•ng dtype: {dtype}")
        
        # Load model
        model = AutoModel.from_pretrained(
            LOCAL_MODEL_PATH,
            torch_dtype=dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_flash_attn=False,
            local_files_only=True
        ).eval().to(device)
        
        print(f"‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng l√™n {device}")

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise

# Load model s·∫Ω ƒë∆∞·ª£c g·ªçi trong __main__ block

# Endpoint root
@app.route('/', methods=['GET'])
def root():
    """Endpoint root ƒë·ªÉ ki·ªÉm tra server."""
    return jsonify({
        "status": "success",
        "message": "InternVL Invoice Extraction API",
        "version": "1.0",
        "endpoints": {
            "health": "/health",
            "extract_invoice": "/extract_invoice"
        }
    }), 200

# Endpoint health check
@app.route('/health', methods=['GET'])
def health():
    """Ki·ªÉm tra tr·∫°ng th√°i server v√† model."""
    model_status = "ready" if (model is not None and tokenizer is not None) else "not_ready"
    
    # Th√¥ng tin device
    device_info = {
        "device": device,
        "cuda_available": torch.cuda.is_available()
    }
    if torch.cuda.is_available():
        device_info["gpu_name"] = torch.cuda.get_device_name(0)
        device_info["gpu_memory"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB"
    
    # Th√¥ng tin queue
    queue_info = {
        "queue_size": request_queue.qsize(),
        "is_processing": processing_lock.locked()
    }
    
    return jsonify({
        "status": "success",
        "server": "running",
        "model_status": model_status,
        "device": device_info,
        "queue": queue_info
    }), 200 if model_status == "ready" else 503

# Question m·∫∑c ƒë·ªãnh cho tr√≠ch xu·∫•t h√≥a ƒë∆°n
DEFAULT_QUESTION = """<image>
Tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c tr∆∞·ªùng th√¥ng tin t·ª´ h√≥a ƒë∆°n/bi√™n lai trong ·∫£nh d∆∞·ªõi d·∫°ng ƒë·ªëi t∆∞·ª£ng JSON.
C√°c tr∆∞·ªùng B·∫ÆT BU·ªòC ph·∫£i tr√≠ch xu·∫•t:
- "T√™n ng∆∞·ªùi b√°n"
- "ƒê·ªãa ch·ªâ"
- "Ng√†y giao d·ªãch"
- "T·ªïng ti·ªÅn thanh to√°n" (Total Amount)
- "Danh s√°ch m√≥n" (M·∫£ng ch·ª©a "T√™n m√≥n", "ƒê∆°n gi√°", "S·ªë l∆∞·ª£ng")
"""

def process_invoice_request(request_id, image_data):
    """X·ª≠ l√Ω request tr√≠ch xu·∫•t h√≥a ƒë∆°n (ch·∫°y trong worker thread)"""
    try:
        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
        pixel_values = load_image(image_data).to(
            torch.bfloat16 if device == "cuda" else torch.float32
        ).to(device)
        
        # C·∫•u h√¨nh Generation (m·∫∑c ƒë·ªãnh)
        generation_config = dict(
            max_new_tokens=1024, 
            do_sample=False,
            temperature=0.0,
            num_beams=3, 
            repetition_penalty=3.5
        )
        
        # Ch·∫°y m√¥ h√¨nh v·ªõi question m·∫∑c ƒë·ªãnh
        with torch.no_grad():
            response = model.chat(tokenizer, pixel_values, DEFAULT_QUESTION, generation_config)

        # L∆∞u k·∫øt qu·∫£ v√† signal event
        with result_lock:
            result_store[request_id] = {
                "status": "success",
                "data": {
                    "extraction_result": response
                }
            }
        
        # Signal event ƒë·ªÉ client bi·∫øt ƒë√£ xong
        with event_lock:
            if request_id in request_events:
                request_events[request_id].set()
    except Exception as e:
        # L∆∞u l·ªói v√† signal event
        with result_lock:
            result_store[request_id] = {
                "status": "error",
                "message": f"L·ªói x·ª≠ l√Ω: {str(e)}"
            }
        
        # Signal event ƒë·ªÉ client bi·∫øt ƒë√£ xong (d√π c√≥ l·ªói)
        with event_lock:
            if request_id in request_events:
                request_events[request_id].set()

def queue_worker():
    """Worker thread x·ª≠ l√Ω request t·ª´ queue"""
    while True:
        try:
            # L·∫•y request t·ª´ queue (blocking)
            request_id, image_data = request_queue.get()
            
            # X·ª≠ l√Ω v·ªõi lock ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ 1 request t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
            with processing_lock:
                print(f"üîÑ ƒêang x·ª≠ l√Ω request {request_id}...")
                process_invoice_request(request_id, image_data)
                print(f"‚úÖ Ho√†n th√†nh request {request_id}")
            
            # ƒê√°nh d·∫•u task ƒë√£ ho√†n th√†nh
            request_queue.task_done()
        except Exception as e:
            print(f"‚ùå L·ªói trong worker thread: {e}")
            import traceback
            traceback.print_exc()

# Worker thread s·∫Ω ƒë∆∞·ª£c kh·ªüi ƒë·ªông sau khi load model (trong __main__)

# API Endpoint Tr√≠ch xu·∫•t H√≥a ƒë∆°n (ch·ªâ c·∫ßn ·∫£nh)
@app.route('/extract_invoice', methods=['POST'])
def extract_invoice():
    """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ h√≥a ƒë∆°n/bi√™n lai. Ch·ªâ c·∫ßn g·ª≠i ·∫£nh."""
    if model is None or tokenizer is None:
        return jsonify({
            "status": "error",
            "message": "Model ch∆∞a s·∫µn s√†ng."
        }), 503

    try:
        image_data = None
        
        # Ki·ªÉm tra xem c√≥ file upload kh√¥ng
        if 'image' in request.files:
            file = request.files['image']
            if file.filename == '':
                return jsonify({
                    "status": "error",
                    "message": "Kh√¥ng c√≥ file ƒë∆∞·ª£c ch·ªçn."
                }), 400
            image_data = file.read()
        
        # N·∫øu kh√¥ng c√≥ file upload, ki·ªÉm tra image_url
        elif request.is_json:
            data = request.get_json()
            
            if not data or 'image_url' not in data:
                return jsonify({
                    "status": "error",
                    "message": "C·∫ßn cung c·∫•p 'image_url' (JSON) ho·∫∑c upload file 'image' (multipart/form-data)."
                }), 400
            
            image_url = data.get('image_url')
            response_img = requests.get(image_url, timeout=10)
            response_img.raise_for_status() 
            image_data = response_img.content
        else:
            return jsonify({
                "status": "error",
                "message": "C·∫ßn cung c·∫•p 'image_url' (JSON) ho·∫∑c upload file 'image' (multipart/form-data)."
            }), 400
        
        if image_data is None:
            return jsonify({
                "status": "error",
                "message": "Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu ·∫£nh."
            }), 400
        
        # T·∫°o request ID v√† Event
        request_id = str(uuid.uuid4())
        request_event = threading.Event()
        
        # L∆∞u event
        with event_lock:
            request_events[request_id] = request_event
        
        # Th√™m v√†o queue
        request_queue.put((request_id, image_data))
        queue_size = request_queue.qsize()
        
        print(f"üì• ƒê√£ th√™m request {request_id} v√†o queue (queue size: {queue_size})")
        
        # ƒê·ª£i k·∫øt qu·∫£ v·ªõi Event (kh√¥ng c·∫ßn polling - hi·ªáu qu·∫£ h∆°n)
        timeout = 300  # 5 ph√∫t timeout
        if request_event.wait(timeout=timeout):
            # Event ƒë∆∞·ª£c signal - request ƒë√£ xong
            with result_lock:
                if request_id in result_store:
                    result = result_store.pop(request_id)
                    status_code = 200 if result.get("status") == "success" else 500
                    
                    # Cleanup event
                    with event_lock:
                        request_events.pop(request_id, None)
                    
                    return jsonify(result), status_code
        
        # Timeout - Event kh√¥ng ƒë∆∞·ª£c signal trong th·ªùi gian ch·ªù
        with event_lock:
            request_events.pop(request_id, None)
        with result_lock:
            result_store.pop(request_id, None)
        
        return jsonify({
            "status": "error",
            "message": "Request timeout - x·ª≠ l√Ω qu√° l√¢u"
        }), 504

    except requests.exceptions.RequestException as e:
        return jsonify({
            "status": "error",
            "message": f"Kh√¥ng th·ªÉ t·∫£i ·∫£nh t·ª´ URL: {str(e)}"
        }), 400
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"L·ªói x·∫£y ra: {str(e)}"
        }), 500

if __name__ == '__main__':
    try:
        load_model()
    except Exception as e:
        import traceback
        traceback.print_exc()
        os._exit(1)
    
    # Kh·ªüi ƒë·ªông worker thread ƒë·ªÉ x·ª≠ l√Ω queue
    worker_thread = threading.Thread(target=queue_worker, daemon=True)
    worker_thread.start()
    print("‚úÖ Queue worker thread ƒë√£ kh·ªüi ƒë·ªông")
    print(f"   Queue system: X·ª≠ l√Ω tu·∫ßn t·ª± (1 request t·∫°i m·ªôt th·ªùi ƒëi·ªÉm)")
    
    # T·ª± ƒë·ªông ph√°t hi·ªán port t·ª´ environment variable
    # Hugging Face Spaces d√πng port 7860, m·∫∑c ƒë·ªãnh l√† 8000
    port = int(os.environ.get('PORT', 8000))
    print(f"üöÄ Starting server on port {port}")
    
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
